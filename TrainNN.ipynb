{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92259ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t, torch.nn as nn, torch.nn.functional as F, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "#import wideresnet # from The Google Research Authors\n",
    "import json\n",
    "from torchvision import datasets\n",
    "from pathlib import Path\n",
    "#from wrn import WRN\n",
    "from zntrack import ZnTrackProject, Node, config, dvc, zn\n",
    "from zntrack.utils.decorators import check_signature\n",
    "\n",
    "# config.nb_Name must be the notebook file name so that ZnTrack can generate the associated src/ .py scripts\n",
    "config.nb_name = \"TrainNN.ipynb\"\n",
    "project = ZnTrackProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  #input tensors [x, 3, 32, 32]\n",
    "            nn.Softplus(), #softplus is a different non-linear activation function, similar to ReLU\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  #input [ x, 32, 32, 32]\n",
    "            nn.Softplus(),  #output [x, 64, 32, 32]\n",
    "            nn.MaxPool2d(2, 2),  #output [x, 64, 16, 16]\n",
    "            nn.BatchNorm2d(64), #, eps=1e-05, momentum=0.3, affine=True, track_running_stats=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #out [1, 128, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), #out [1, 128, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), # out [1, 256, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), # out [1, 256, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bc141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    def compute(self, inp):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cac777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to define ML training workflow through ZnTrack\n",
    "# 1. create computation class extending Base, implement compute function\n",
    "\n",
    "class Trainer(Base):\n",
    "    @check_signature\n",
    "    def __init__(self):\n",
    "        self.model = WRN()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = t.optim.Adam(self.model.parameters(), lr=1e-3,weight_decay=0.0005)\n",
    "        #self.dataloader = dataloader\n",
    "        self.device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def train(self, model):\n",
    "        transform_normal = tr.Compose([tr.ToTensor(), tr.Normalize((.49, .48, .44), (.24, .24, .26))])\n",
    "        normal_train = tv.datasets.CIFAR10(root='root', transform=transform_normal, download=False, train=True)\n",
    "        train_ds, val_ds = random_split(normal_train, [45000, 5000])\n",
    "        batch_size = 64 \n",
    "        dataloader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        #val_dl = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = t.optim.Adam(model.parameters(), lr=1e-3,weight_decay=0.0005)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        #self.dataloader = train_dl\n",
    "        \n",
    "        size = len(dataloader.dataset)\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            #X is the batch of images, y is the vector of numeric labels for them\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch % 100 == 0:\n",
    "                #print(y)\n",
    "                #print(pred)\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            \n",
    "    def test(self, model):\n",
    "        transform_normal = tr.Compose([tr.ToTensor(), tr.Normalize((.49, .48, .44), (.24, .24, .26))])\n",
    "        normal_train = tv.datasets.CIFAR10(root='root', transform=transform_normal, download=False, train=True)\n",
    "        train_ds, val_ds = random_split(normal_train, [45000, 5000])\n",
    "        batch_size = 64 \n",
    "        #train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        dataloader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory=True)\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = model(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        acc = 100*correct\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        return acc, test_loss   \n",
    "    \n",
    "    def dotraining(self, epochs):\n",
    "        #model = self.model\n",
    "        #model\n",
    "        test_scores = {}\n",
    "        for step in range(epochs):\n",
    "            print(f\"Epoch {step+1}\\n-------------------------------\")\n",
    "            test_scores[step] = {}\n",
    "            self.train(self.model)\n",
    "            testAcc, testLoss = self.test(self.model)\n",
    "            test_scores[step] = {\"acc:\": float(testAcc), \"loss\": float(testLoss)}\n",
    "        with open( self.resultsfile, 'w') as outfile:\n",
    "            json.dump(test_scores, outfile)\n",
    "        \n",
    "    \n",
    "    def compute(self, inp):\n",
    "        #raise NotImplementedError\n",
    "        self.dotraining(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.  Create Node class, implement __call__ and run, optionally __init__\n",
    "# use the zn.Method() type for the Base class extention \n",
    "# this builds the py script, so run after any changes at all\n",
    "\n",
    "@Node()\n",
    "class Train:\n",
    "    epochs = dvc.params()\n",
    "    trainer: Base = zn.Method()\n",
    "    resultsfile: Path = dvc.outs()\n",
    "    result = zn.outs()\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = WRN()\n",
    "        \n",
    "    def __call__(self, epochs, resultsfile: Path, trainer):\n",
    "        self.trainer = trainer\n",
    "        self.epochs = epochs\n",
    "        self.resultsfile = resultsfile\n",
    "        self.resultsfile.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "    def run(self):\n",
    "        self.result = self.trainer.compute(inp=self.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.  instantiate classes, pass the base compute class into the node class \n",
    "# this creates and writes the dvc.yaml stages \n",
    "\n",
    "trainer = Trainer()\n",
    "\n",
    "train_stage = Train()\n",
    "train_stage(epochs=3, resultsfile=Path(\"outs\", \"test_scores.json\"), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf93b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dvc repro\n",
    "# equivalent to this\n",
    "project.repro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e073a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
